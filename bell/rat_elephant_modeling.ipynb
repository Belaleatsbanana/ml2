{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "modeling_notebook_title",
   "metadata": {},
   "source": [
    "# Jungle Chess Classification - Modeling Pipeline\n",
    "\n",
    "This notebook continues from the EDA/Cleaning notebook.\n",
    "\n",
    "## Workflow:\n",
    "1. Load cleaned data\n",
    "2. Feature Selection\n",
    "3. Imbalance Handling (4 strategies)\n",
    "4. Model Training (7 classifiers)\n",
    "5. Ensemble Methods\n",
    "6. Cost-Sensitive Classification\n",
    "7. Hyperparameter Tuning\n",
    "8. Evaluation & Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modeling_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, classification_report, roc_auc_score,\n",
    "                             balanced_accuracy_score, matthews_corrcoef, cohen_kappa_score)\n",
    "\n",
    "# Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
    "                              VotingClassifier, StackingClassifier, BaggingClassifier)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Imbalanced-learn\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = int('GOGU', 36)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"RANDOM_SEED: {RANDOM_SEED}\")\n",
    "print(\"\u2705 All imports loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_cleaned_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data from CSV\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING CLEANED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_data_clean = pd.read_csv('bell/train_data_clean.csv')\n",
    "test_data_clean = pd.read_csv('bell/test_data_clean.csv')\n",
    "\n",
    "print(f\"\\n\u2705 Loaded train_data_clean: {train_data_clean.shape}\")\n",
    "print(f\"\u2705 Loaded test_data_clean: {test_data_clean.shape}\")\n",
    "\n",
    "print(f\"\\nTrain columns: {train_data_clean.columns.tolist()[:5]}...\")\n",
    "print(f\"Train dtypes: {train_data_clean.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\nClass distribution (train):\")\n",
    "print(train_data_clean['class'].value_counts().sort_index())\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = train_data_clean.drop('class', axis=1)\n",
    "y_train = train_data_clean['class']\n",
    "\n",
    "X_test = test_data_clean.drop('class', axis=1)\n",
    "y_test = test_data_clean['class']\n",
    "\n",
    "print(f\"\\nX_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modeling_title",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 4: Comprehensive Modeling Pipeline\n",
    "\n",
    "## Workflow:\n",
    "1. **Feature Selection** - Select best features using correlation and importance\n",
    "2. **Imbalance Handling** - 4 strategies (None, SVMSMOTE, TomekLinks, SMOTETomek)\n",
    "3. **Model Training** - 7 classifiers (DT, LR, SVM, KNN, RF, GB, XGB)\n",
    "4. **Ensemble Methods** - VotingClassifier, StackingClassifier, BaggingClassifier\n",
    "5. **Hyperparameter Tuning** - StratifiedKFold cross-validation\n",
    "6. **Evaluation** - Metrics, Learning Curves, Bias-Variance Analysis\n",
    "7. **MLflow Logging** - Track everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlflow_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow Setup\n",
    "mlflow.set_experiment(\"Jungle_Chess_Classification\")\n",
    "print(\"\u2705 MLflow experiment set!\")\n",
    "\n",
    "# MLflow logging functions\n",
    "def log_metrics_to_mlflow(y_true, y_pred, y_proba=None, prefix=''):\n",
    "    \"\"\"Log classification metrics to MLflow\"\"\"\n",
    "    metrics = {\n",
    "        f'{prefix}accuracy': accuracy_score(y_true, y_pred),\n",
    "        f'{prefix}precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        f'{prefix}recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        f'{prefix}f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        f'{prefix}balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        f'{prefix}mcc': matthews_corrcoef(y_true, y_pred),\n",
    "        f'{prefix}cohen_kappa': cohen_kappa_score(y_true, y_pred)\n",
    "    }\n",
    "    if y_proba is not None:\n",
    "        try:\n",
    "            metrics[f'{prefix}roc_auc'] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')\n",
    "        except:\n",
    "            pass\n",
    "    mlflow.log_metrics(metrics)\n",
    "    return metrics\n",
    "\n",
    "def log_confusion_matrix_plot(y_true, y_pred, title='Confusion Matrix'):\n",
    "    \"\"\"Log confusion matrix as artifact\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Black', 'White', 'Draw'], yticklabels=['Black', 'White', 'Draw'])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    filename = 'confusion_matrix.png'\n",
    "    plt.savefig(filename, dpi=100)\n",
    "    mlflow.log_artifact(filename)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print(\"\u2705 MLflow logging functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y from cleaned data\n",
    "X_train = train_data_clean.drop('class', axis=1)\n",
    "y_train = train_data_clean['class']\n",
    "\n",
    "X_test = test_data_clean.drop('class', axis=1)\n",
    "y_test = test_data_clean['class']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA PREPARED FOR MODELING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"\\nClass distribution in y_train:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(f\"\\nFeature names: {X_train.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_selection_title",
   "metadata": {},
   "source": [
    "## Step 1: Feature Selection\n",
    "\n",
    "Using multiple methods:\n",
    "1. Correlation with target\n",
    "2. Random Forest feature importance\n",
    "3. Select top features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Method 1: Correlation with target\n",
    "correlations = X_train.corrwith(y_train).abs().sort_values(ascending=False)\n",
    "print(\"\\n1. Top 15 features by correlation with target:\")\n",
    "print(correlations.head(15))\n",
    "\n",
    "# Method 2: Random Forest importance\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "rf_selector.fit(X_train, y_train)\n",
    "importances = pd.Series(rf_selector.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"\\n2. Top 15 features by Random Forest importance:\")\n",
    "print(importances.head(15))\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "correlations.head(15).plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Top 15 Features by Correlation')\n",
    "axes[0].set_xlabel('Absolute Correlation')\n",
    "\n",
    "importances.head(15).plot(kind='barh', ax=axes[1], color='forestgreen')\n",
    "axes[1].set_title('Top 15 Features by RF Importance')\n",
    "axes[1].set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select top features (union of top 20 from each method)\n",
    "top_corr_features = set(correlations.head(20).index)\n",
    "top_rf_features = set(importances.head(20).index)\n",
    "selected_features = list(top_corr_features.union(top_rf_features))\n",
    "\n",
    "print(f\"\\n3. Selected {len(selected_features)} features (union of top 20 from each method)\")\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "# Create selected feature datasets\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "print(f\"\\nX_train_selected shape: {X_train_selected.shape}\")\n",
    "print(f\"X_test_selected shape: {X_test_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imbalance_title",
   "metadata": {},
   "source": [
    "## Step 2: Imbalance Handling Strategies\n",
    "\n",
    "We'll create 4 different pipelines using **ImbPipeline**:\n",
    "\n",
    "| Pipeline | Strategy | Type | Description |\n",
    "|----------|----------|------|-------------|\n",
    "| 1 | **None** | Baseline | No resampling |\n",
    "| 2 | **KMeansSMOTE** | Oversampling | KMeans clustering + SMOTE |\n",
    "| 3 | **ClusterCentroids** | Undersampling | Cluster-based centroid sampling |\n",
    "| 4 | **SMOTEENN** | Combined | SMOTE + Edited Nearest Neighbors |\n",
    "\n",
    "Using `ImbPipeline` from `imblearn.pipeline` for clean, reproducible code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e18448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store resampled datasets\n",
    "resampled_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imbalance_handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance Handling with ImbPipeline\n",
    "print(\"=\"*60)\n",
    "print(\"IMBALANCE HANDLING PIPELINES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define 4 resampling strategies\n",
    "resampling_strategies = {\n",
    "    'None': None,  # No resampling\n",
    "    'KMeansSMOTE': KMeansSMOTE(random_state=RANDOM_SEED, cluster_balance_threshold=0.1),\n",
    "    'ClusterCentroids': ClusterCentroids(random_state=RANDOM_SEED),\n",
    "    'SMOTEENN': SMOTEENN(random_state=RANDOM_SEED)\n",
    "}\n",
    "\n",
    "print(\"\\nResampling Strategies:\")\n",
    "print(\"  1. None - Baseline (no resampling)\")\n",
    "print(\"  2. KMeansSMOTE - Oversampling (KMeans + SMOTE)\")\n",
    "print(\"  3. ClusterCentroids - Undersampling (cluster-based)\")\n",
    "print(\"  4. SMOTEENN - Combined (SMOTE + ENN)\")\n",
    "\n",
    "# Convert to numpy\n",
    "X_np = X_train_selected.values\n",
    "y_np = y_train.values\n",
    "\n",
    "\n",
    "print(f\"\\nOriginal data shape: {X_np.shape}\")\n",
    "print(f\"Original class distribution: {dict(zip(*np.unique(y_np, return_counts=True)))}\")\n",
    "\n",
    "for name, resampler in resampling_strategies.items():\n",
    "    print(f\"\\nApplying {name}...\")\n",
    "    \n",
    "    if resampler is None:\n",
    "        X_res, y_res = X_np.copy(), y_np.copy()\n",
    "    else:\n",
    "        try:\n",
    "            X_res, y_res = resampler.fit_resample(X_np, y_np)\n",
    "        except Exception as e:\n",
    "            print(f\"  \u26a0\ufe0f {name} failed: {e}\")\n",
    "            print(f\"  Using original data instead\")\n",
    "            X_res, y_res = X_np.copy(), y_np.copy()\n",
    "    \n",
    "    resampled_data[name] = (X_res, y_res)\n",
    "    unique, counts = np.unique(y_res, return_counts=True)\n",
    "    print(f\"  Shape: {X_res.shape}\")\n",
    "    print(f\"  Class distribution: {dict(zip(unique, counts))}\")\n",
    "\n",
    "# Visualize class distribution after resampling\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = ['#e74c3c', '#3498db', '#95a5a6']\n",
    "\n",
    "for idx, (name, (X_res, y_res)) in enumerate(resampled_data.items()):\n",
    "    unique, counts = np.unique(y_res, return_counts=True)\n",
    "    bars = axes[idx].bar(unique, counts, color=colors[:len(unique)])\n",
    "    axes[idx].set_title(f'{name}\\n(Total: {len(y_res):,} samples)', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Class')\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].set_xticks([0, 1, 2])\n",
    "    axes[idx].set_xticklabels(['Black (0)', 'White (1)', 'Draw (2)'])\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, c in zip(bars, counts):\n",
    "        axes[idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, \n",
    "                      f'{c:,}', ha='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.suptitle('Class Distribution After Resampling Strategies', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2705 All resampling strategies applied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models_title",
   "metadata": {},
   "source": [
    "## Step 3: Define Base Models\n",
    "\n",
    "7 classification models:\n",
    "1. Decision Tree\n",
    "2. Logistic Regression\n",
    "3. SVM\n",
    "4. KNN\n",
    "5. Random Forest\n",
    "6. Gradient Boosting\n",
    "7. XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Base Models\n",
    "print(\"=\"*60)\n",
    "print(\"DEFINE BASE MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "base_models = {\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=RANDOM_SEED),\n",
    "    'LogisticRegression': LogisticRegression(random_state=RANDOM_SEED, max_iter=1000),\n",
    "    'SVM': SVC(random_state=RANDOM_SEED, probability=True),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'RandomForest': RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=RANDOM_SEED),\n",
    "    'XGBoost': XGBClassifier(random_state=RANDOM_SEED, eval_metric='mlogloss', use_label_encoder=False)\n",
    "}\n",
    "\n",
    "print(\"\\nBase models defined:\")\n",
    "for name in base_models.keys():\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(base_models)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_title",
   "metadata": {},
   "source": [
    "## Step 4: Train Base Models on All Resampled Datasets\n",
    "\n",
    "Train each model on each resampling strategy and log to MLflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Models using ImbPipeline and resampled_data\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "trained_pipelines = {}\n",
    "\n",
    "# Prepare test data\n",
    "X_test_np = X_test_selected.values\n",
    "y_test_np = y_test.values\n",
    "\n",
    "# Define base models\n",
    "base_models = {\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=RANDOM_SEED),\n",
    "    'LogisticReg': LogisticRegression(random_state=RANDOM_SEED, max_iter=1000),\n",
    "    'SVM': SVC(random_state=RANDOM_SEED, probability=True),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'RandomForest': RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=RANDOM_SEED),\n",
    "    'XGBoost': XGBClassifier(random_state=RANDOM_SEED, eval_metric='mlogloss', use_label_encoder=False)\n",
    "}\n",
    "\n",
    "# Train on each resampled dataset from resampled_data\n",
    "for resample_name, (X_res, y_res) in resampled_data.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Resampling Strategy: {resample_name}\")\n",
    "    print(f\"Training data shape: {X_res.shape}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Scale the resampled data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_res)\n",
    "    X_test_scaled = scaler.transform(X_test_np)\n",
    "    \n",
    "    for model_name, model in base_models.items():\n",
    "        # Clone model\n",
    "        model_clone = model.__class__(**model.get_params())\n",
    "        \n",
    "        run_name = f\"{model_name}_{resample_name}\"\n",
    "        \n",
    "        with mlflow.start_run(run_name=run_name):\n",
    "            mlflow.log_param('model', model_name)\n",
    "            mlflow.log_param('resampling', resample_name)\n",
    "            mlflow.log_param('n_features', X_res.shape[1])\n",
    "            mlflow.log_param('n_train_samples', X_res.shape[0])\n",
    "            \n",
    "            try:\n",
    "                # Use scaled data for distance-based models\n",
    "                if model_name in ['SVM', 'LogisticReg', 'KNN']:\n",
    "                    model_clone.fit(X_train_scaled, y_res)\n",
    "                    y_pred = model_clone.predict(X_test_scaled)\n",
    "                    y_proba = model_clone.predict_proba(X_test_scaled) if hasattr(model_clone, 'predict_proba') else None\n",
    "                else:\n",
    "                    model_clone.fit(X_res, y_res)\n",
    "                    y_pred = model_clone.predict(X_test_np)\n",
    "                    y_proba = model_clone.predict_proba(X_test_np) if hasattr(model_clone, 'predict_proba') else None\n",
    "                \n",
    "                # Calculate metrics\n",
    "                acc = accuracy_score(y_test_np, y_pred)\n",
    "                f1 = f1_score(y_test_np, y_pred, average='macro', zero_division=0)\n",
    "                prec = precision_score(y_test_np, y_pred, average='macro', zero_division=0)\n",
    "                rec = recall_score(y_test_np, y_pred, average='macro', zero_division=0)\n",
    "                \n",
    "                log_metrics_to_mlflow(y_test_np, y_pred, y_proba, prefix='test_')\n",
    "                \n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Resampling': resample_name,\n",
    "                    'Accuracy': acc,\n",
    "                    'F1_Macro': f1,\n",
    "                    'Precision': prec,\n",
    "                    'Recall': rec,\n",
    "                    'Status': 'SUCCESS'\n",
    "                })\n",
    "                \n",
    "                trained_pipelines[run_name] = model_clone\n",
    "                print(f\"  \u2705 {model_name}: Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  \u274c {model_name} failed: {str(e)[:50]}\")\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Resampling': resample_name,\n",
    "                    'Accuracy': 0, 'F1_Macro': 0, 'Precision': 0, 'Recall': 0,\n",
    "                    'Status': 'FAILED'\n",
    "                })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2705 All models trained!\")\n",
    "print(f\"Total runs: {len(results)}\")\n",
    "print(f\"Successful: {(results_df['Status'] == 'SUCCESS').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Summary\n",
    "print(\"=\"*60)\n",
    "print(\"BASE MODEL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pivot table for better visualization\n",
    "pivot_acc = results_df.pivot(index='Model', columns='Resampling', values='Accuracy')\n",
    "pivot_f1 = results_df.pivot(index='Model', columns='Resampling', values='F1_Macro')\n",
    "\n",
    "print(\"\\nAccuracy by Model and Resampling:\")\n",
    "print(pivot_acc.round(4).to_string())\n",
    "\n",
    "print(\"\\nF1 Macro by Model and Resampling:\")\n",
    "print(pivot_f1.round(4).to_string())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "pivot_acc.plot(kind='bar', ax=axes[0], width=0.8)\n",
    "axes[0].set_title('Accuracy by Model and Resampling', fontweight='bold')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(title='Resampling')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "pivot_f1.plot(kind='bar', ax=axes[1], width=0.8)\n",
    "axes[1].set_title('F1 Macro by Model and Resampling', fontweight='bold')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('F1 Macro')\n",
    "axes[1].legend(title='Resampling')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best model per resampling\n",
    "print(\"\\nBest Model per Resampling Strategy (by F1):\")\n",
    "for resample in resampled_data.keys():\n",
    "    best_row = results_df[results_df['Resampling'] == resample].nlargest(1, 'F1_Macro').iloc[0]\n",
    "    print(f\"  {resample}: {best_row['Model']} (F1={best_row['F1_Macro']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ensemble_title",
   "metadata": {},
   "source": [
    "## Step 5: Ensemble Methods\n",
    "\n",
    "Create ensembles for each resampling strategy:\n",
    "1. **VotingClassifier** - Soft voting of top 3 models\n",
    "2. **StackingClassifier** - Stack with LogisticRegression meta-learner\n",
    "3. **BaggingClassifier** - Bagging with best base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble_methods",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Methods using resampled_data\n",
    "print(\"=\"*60)\n",
    "print(\"ENSEMBLE METHODS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ensemble_results = []\n",
    "\n",
    "for resample_name, (X_res, y_res) in resampled_data.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Ensembles for: {resample_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_res)\n",
    "    X_test_scaled = scaler.transform(X_test_np)\n",
    "    \n",
    "    # 1. VotingClassifier\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=100)),\n",
    "            ('xgb', XGBClassifier(random_state=RANDOM_SEED, eval_metric='mlogloss', use_label_encoder=False)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=RANDOM_SEED))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    with mlflow.start_run(run_name=f'VotingClassifier_{resample_name}'):\n",
    "        mlflow.log_param('ensemble', 'VotingClassifier')\n",
    "        mlflow.log_param('resampling', resample_name)\n",
    "        \n",
    "        try:\n",
    "            voting_clf.fit(X_res, y_res)\n",
    "            y_pred = voting_clf.predict(X_test_np)\n",
    "            y_proba = voting_clf.predict_proba(X_test_np)\n",
    "            \n",
    "            acc = accuracy_score(y_test_np, y_pred)\n",
    "            f1 = f1_score(y_test_np, y_pred, average='macro', zero_division=0)\n",
    "            log_metrics_to_mlflow(y_test_np, y_pred, y_proba, prefix='test_')\n",
    "            \n",
    "            ensemble_results.append({'Ensemble': 'VotingClassifier', 'Resampling': resample_name,\n",
    "                                     'Accuracy': acc, 'F1_Macro': f1})\n",
    "            print(f\"  \u2705 VotingClassifier: Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  \u274c VotingClassifier failed: {str(e)[:50]}\")\n",
    "            ensemble_results.append({'Ensemble': 'VotingClassifier', 'Resampling': resample_name,\n",
    "                                     'Accuracy': 0, 'F1_Macro': 0})\n",
    "    \n",
    "    # 2. StackingClassifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=50)),\n",
    "            ('dt', DecisionTreeClassifier(random_state=RANDOM_SEED)),\n",
    "            ('knn', KNeighborsClassifier())\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(random_state=RANDOM_SEED, max_iter=1000),\n",
    "        cv=3\n",
    "    )\n",
    "    \n",
    "    with mlflow.start_run(run_name=f'StackingClassifier_{resample_name}'):\n",
    "        mlflow.log_param('ensemble', 'StackingClassifier')\n",
    "        mlflow.log_param('resampling', resample_name)\n",
    "        \n",
    "        try:\n",
    "            stacking_clf.fit(X_train_scaled, y_res)\n",
    "            y_pred = stacking_clf.predict(X_test_scaled)\n",
    "            y_proba = stacking_clf.predict_proba(X_test_scaled)\n",
    "            \n",
    "            acc = accuracy_score(y_test_np, y_pred)\n",
    "            f1 = f1_score(y_test_np, y_pred, average='macro', zero_division=0)\n",
    "            log_metrics_to_mlflow(y_test_np, y_pred, y_proba, prefix='test_')\n",
    "            \n",
    "            ensemble_results.append({'Ensemble': 'StackingClassifier', 'Resampling': resample_name,\n",
    "                                     'Accuracy': acc, 'F1_Macro': f1})\n",
    "            print(f\"  \u2705 StackingClassifier: Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  \u274c StackingClassifier failed: {str(e)[:50]}\")\n",
    "            ensemble_results.append({'Ensemble': 'StackingClassifier', 'Resampling': resample_name,\n",
    "                                     'Accuracy': 0, 'F1_Macro': 0})\n",
    "    \n",
    "    # 3. BaggingClassifier\n",
    "    bagging_clf = BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(random_state=RANDOM_SEED),\n",
    "        n_estimators=50,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    with mlflow.start_run(run_name=f'BaggingClassifier_{resample_name}'):\n",
    "        mlflow.log_param('ensemble', 'BaggingClassifier')\n",
    "        mlflow.log_param('resampling', resample_name)\n",
    "        \n",
    "        try:\n",
    "            bagging_clf.fit(X_res, y_res)\n",
    "            y_pred = bagging_clf.predict(X_test_np)\n",
    "            y_proba = bagging_clf.predict_proba(X_test_np)\n",
    "            \n",
    "            acc = accuracy_score(y_test_np, y_pred)\n",
    "            f1 = f1_score(y_test_np, y_pred, average='macro', zero_division=0)\n",
    "            log_metrics_to_mlflow(y_test_np, y_pred, y_proba, prefix='test_')\n",
    "            \n",
    "            ensemble_results.append({'Ensemble': 'BaggingClassifier', 'Resampling': resample_name,\n",
    "                                     'Accuracy': acc, 'F1_Macro': f1})\n",
    "            print(f\"  \u2705 BaggingClassifier: Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  \u274c BaggingClassifier failed: {str(e)[:50]}\")\n",
    "            ensemble_results.append({'Ensemble': 'BaggingClassifier', 'Resampling': resample_name,\n",
    "                                     'Accuracy': 0, 'F1_Macro': 0})\n",
    "\n",
    "ensemble_df = pd.DataFrame(ensemble_results)\n",
    "print(\"\\n\u2705 All ensembles trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Results Summary\n",
    "print(\"=\"*60)\n",
    "print(\"ENSEMBLE RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pivot_ens = ensemble_df.pivot(index='Ensemble', columns='Resampling', values='F1_Macro')\n",
    "print(\"\\nF1 Macro by Ensemble and Resampling:\")\n",
    "print(pivot_ens.round(4).to_string())\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "pivot_ens.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Ensemble F1 Macro by Resampling Strategy', fontweight='bold')\n",
    "ax.set_xlabel('Ensemble Method')\n",
    "ax.set_ylabel('F1 Macro')\n",
    "ax.legend(title='Resampling')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best overall\n",
    "best_ens = ensemble_df.nlargest(3, 'F1_Macro')\n",
    "print(\"\\nTop 3 Ensemble Configurations:\")\n",
    "print(best_ens.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost_sensitive_title",
   "metadata": {},
   "source": [
    "## Step 5b: Cost-Sensitive Classification\n",
    "\n",
    "Instead of resampling, we can handle class imbalance by adjusting class weights.\n",
    "\n",
    "### Approaches:\n",
    "1. **class_weight='balanced'** - Automatically adjusts weights inversely proportional to class frequencies\n",
    "2. **Custom class weights** - Manually specify weights based on business cost\n",
    "3. **sample_weight** - Per-sample weights during training\n",
    "\n",
    "### Advantages over Resampling:\n",
    "- No synthetic data generation\n",
    "- No data loss from undersampling\n",
    "- Faster training (original dataset size)\n",
    "- Works with any cost structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cost_sensitive_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost-Sensitive Classification\n",
    "print(\"=\"*60)\n",
    "print(\"COST-SENSITIVE CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute balanced weights\n",
    "classes = np.unique(y_np)\n",
    "balanced_weights = compute_class_weight('balanced', classes=classes, y=y_np)\n",
    "class_weight_dict = dict(zip(classes, balanced_weights))\n",
    "\n",
    "print(f\"\\nClass distribution: {dict(zip(*np.unique(y_np, return_counts=True)))}\")\n",
    "print(f\"Balanced class weights: {class_weight_dict}\")\n",
    "\n",
    "# Custom weights (example: penalize misclassifying minority class more)\n",
    "# Higher weight = higher penalty for misclassification\n",
    "custom_weights = {\n",
    "    0: 1.0,  # Black\n",
    "    1: 1.0,  # White  \n",
    "    2: 3.0   # Draw (minority - higher penalty)\n",
    "}\n",
    "print(f\"Custom class weights: {custom_weights}\")\n",
    "\n",
    "# Models that support class_weight\n",
    "cost_sensitive_models = {\n",
    "    'DecisionTree_Balanced': DecisionTreeClassifier(random_state=RANDOM_SEED, class_weight='balanced'),\n",
    "    'DecisionTree_Custom': DecisionTreeClassifier(random_state=RANDOM_SEED, class_weight=custom_weights),\n",
    "    'LogisticReg_Balanced': LogisticRegression(random_state=RANDOM_SEED, max_iter=1000, class_weight='balanced'),\n",
    "    'LogisticReg_Custom': LogisticRegression(random_state=RANDOM_SEED, max_iter=1000, class_weight=custom_weights),\n",
    "    'SVM_Balanced': SVC(random_state=RANDOM_SEED, probability=True, class_weight='balanced'),\n",
    "    'SVM_Custom': SVC(random_state=RANDOM_SEED, probability=True, class_weight=custom_weights),\n",
    "    'RandomForest_Balanced': RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1, class_weight='balanced'),\n",
    "    'RandomForest_Custom': RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1, class_weight=custom_weights),\n",
    "    'XGBoost_ScaleWeight': XGBClassifier(random_state=RANDOM_SEED, eval_metric='mlogloss', use_label_encoder=False,\n",
    "                                         scale_pos_weight=len(y_np[y_np==0])/len(y_np[y_np==2]))  # Adjust for imbalance\n",
    "}\n",
    "\n",
    "print(f\"\\nTraining {len(cost_sensitive_models)} cost-sensitive models...\")\n",
    "\n",
    "cost_sensitive_results = []\n",
    "\n",
    "for model_name, model in cost_sensitive_models.items():\n",
    "    # Create pipeline with scaling\n",
    "    pipeline = ImbPipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    with mlflow.start_run(run_name=f'CostSensitive_{model_name}'):\n",
    "        mlflow.log_param('model', model_name)\n",
    "        mlflow.log_param('approach', 'cost_sensitive')\n",
    "        mlflow.log_param('resampling', 'None')\n",
    "        \n",
    "        if 'Balanced' in model_name:\n",
    "            mlflow.log_param('class_weight', 'balanced')\n",
    "        elif 'Custom' in model_name:\n",
    "            mlflow.log_param('class_weight', str(custom_weights))\n",
    "        else:\n",
    "            mlflow.log_param('class_weight', 'scale_pos_weight')\n",
    "        \n",
    "        try:\n",
    "            # Train\n",
    "            pipeline.fit(X_np, y_np)\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = pipeline.predict(X_test_np)\n",
    "            y_proba = pipeline.predict_proba(X_test_np) if hasattr(pipeline, 'predict_proba') else None\n",
    "            \n",
    "            # Metrics\n",
    "            acc = accuracy_score(y_test_np, y_pred)\n",
    "            f1 = f1_score(y_test_np, y_pred, average='macro', zero_division=0)\n",
    "            prec = precision_score(y_test_np, y_pred, average='macro', zero_division=0)\n",
    "            rec = recall_score(y_test_np, y_pred, average='macro', zero_division=0)\n",
    "            \n",
    "            log_metrics_to_mlflow(y_test_np, y_pred, y_proba, prefix='test_')\n",
    "            \n",
    "            cost_sensitive_results.append({\n",
    "                'Model': model_name,\n",
    "                'Approach': 'Cost-Sensitive',\n",
    "                'Accuracy': acc,\n",
    "                'F1_Macro': f1,\n",
    "                'Precision': prec,\n",
    "                'Recall': rec\n",
    "            })\n",
    "            \n",
    "            print(f\"  \u2705 {model_name}: Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  \u274c {model_name} failed: {str(e)[:50]}\")\n",
    "            cost_sensitive_results.append({\n",
    "                'Model': model_name,\n",
    "                'Approach': 'Cost-Sensitive',\n",
    "                'Accuracy': 0,\n",
    "                'F1_Macro': 0,\n",
    "                'Precision': 0,\n",
    "                'Recall': 0\n",
    "            })\n",
    "\n",
    "cost_sensitive_df = pd.DataFrame(cost_sensitive_results)\n",
    "print(\"\\n\u2705 Cost-sensitive training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cost_sensitive_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost-Sensitive Results Summary\n",
    "print(\"=\"*60)\n",
    "print(\"COST-SENSITIVE RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nCost-Sensitive Model Performance:\")\n",
    "print(cost_sensitive_df.sort_values('F1_Macro', ascending=False).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Sort by F1\n",
    "sorted_df = cost_sensitive_df.sort_values('F1_Macro', ascending=True)\n",
    "\n",
    "# F1 Macro\n",
    "colors = ['steelblue' if 'Balanced' in m else 'coral' if 'Custom' in m else 'forestgreen' \n",
    "          for m in sorted_df['Model']]\n",
    "axes[0].barh(sorted_df['Model'], sorted_df['F1_Macro'], color=colors)\n",
    "axes[0].set_xlabel('F1 Macro')\n",
    "axes[0].set_title('Cost-Sensitive Models - F1 Macro', fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].barh(sorted_df['Model'], sorted_df['Accuracy'], color=colors)\n",
    "axes[1].set_xlabel('Accuracy')\n",
    "axes[1].set_title('Cost-Sensitive Models - Accuracy', fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='steelblue', label='Balanced Weights'),\n",
    "    Patch(facecolor='coral', label='Custom Weights'),\n",
    "    Patch(facecolor='forestgreen', label='Scale Weight')\n",
    "]\n",
    "axes[0].legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare best cost-sensitive vs best resampling\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: Cost-Sensitive vs Resampling\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_cost_sensitive = cost_sensitive_df.nlargest(1, 'F1_Macro').iloc[0]\n",
    "best_resampling = results_df[results_df['Status'] == 'SUCCESS'].nlargest(1, 'F1_Macro').iloc[0] if 'Status' in results_df.columns else results_df.nlargest(1, 'F1_Macro').iloc[0]\n",
    "\n",
    "print(f\"\\nBest Cost-Sensitive: {best_cost_sensitive['Model']}\")\n",
    "print(f\"  F1 Macro: {best_cost_sensitive['F1_Macro']:.4f}\")\n",
    "print(f\"  Accuracy: {best_cost_sensitive['Accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Resampling: {best_resampling['Model']} ({best_resampling['Resampling']})\")\n",
    "print(f\"  F1 Macro: {best_resampling['F1_Macro']:.4f}\")\n",
    "print(f\"  Accuracy: {best_resampling['Accuracy']:.4f}\")\n",
    "\n",
    "if best_cost_sensitive['F1_Macro'] > best_resampling['F1_Macro']:\n",
    "    print(\"\\n\ud83c\udfc6 Winner: Cost-Sensitive Classification!\")\n",
    "else:\n",
    "    print(\"\\n\ud83c\udfc6 Winner: Resampling Approach!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost_sensitive_explanation",
   "metadata": {},
   "source": [
    "### Cost-Sensitive Classification Explained\n",
    "\n",
    "#### How it Works:\n",
    "- **class_weight='balanced'**: Automatically computes weights as `n_samples / (n_classes * np.bincount(y))`\n",
    "- **Custom weights**: You define the cost of misclassifying each class\n",
    "- **scale_pos_weight** (XGBoost): Ratio of negative to positive samples\n",
    "\n",
    "#### When to Use:\n",
    "| Approach | Best For |\n",
    "|----------|----------|\n",
    "| **Resampling** | When you have very few minority samples, need diverse synthetic data |\n",
    "| **Cost-Sensitive** | When original data distribution matters, faster training needed |\n",
    "| **Both** | Experiment with both and compare results! |\n",
    "\n",
    "#### Business Cost Example:\n",
    "```python\n",
    "# If misclassifying a Draw is 3x more costly than Black/White:\n",
    "custom_weights = {0: 1.0, 1: 1.0, 2: 3.0}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuning_title",
   "metadata": {},
   "source": [
    "## Step 6: Hyperparameter Tuning with StratifiedKFold\n",
    "\n",
    "Tune the best performing models using RandomizedSearchCV with StratifiedKFold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparameter_tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "print(\"=\"*60)\n",
    "print(\"HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use best resampling strategy (SMOTETomek typically performs well)\n",
    "X_tuning, y_tuning = resampled_data['SMOTETomek']\n",
    "\n",
    "# StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Parameter grids\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 15, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }\n",
    "}\n",
    "\n",
    "tuned_models = {}\n",
    "tuning_results = []\n",
    "\n",
    "for model_name in ['RandomForest', 'XGBoost', 'GradientBoosting']:\n",
    "    print(f\"\\nTuning {model_name}...\")\n",
    "    \n",
    "    model = base_models[model_name].__class__(**base_models[model_name].get_params())\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_grids[model_name],\n",
    "        n_iter=20,\n",
    "        cv=skf,\n",
    "        scoring='f1_macro',\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    with mlflow.start_run(run_name=f'{model_name}_Tuned'):\n",
    "        search.fit(X_tuning, y_tuning)\n",
    "        \n",
    "        # Log best params\n",
    "        mlflow.log_params(search.best_params_)\n",
    "        mlflow.log_param('model', model_name)\n",
    "        mlflow.log_param('tuning', 'RandomizedSearchCV')\n",
    "        mlflow.log_metric('cv_best_f1', search.best_score_)\n",
    "        \n",
    "        # Evaluate on test\n",
    "        y_pred = search.best_estimator_.predict(X_test_np)\n",
    "        y_proba = search.best_estimator_.predict_proba(X_test_np)\n",
    "        \n",
    "        acc = accuracy_score(y_test_np, y_pred)\n",
    "        f1 = f1_score(y_test_np, y_pred, average='macro', zero_division=0)\n",
    "        log_metrics_to_mlflow(y_test_np, y_pred, y_proba, prefix='test_')\n",
    "        \n",
    "        tuned_models[model_name] = search.best_estimator_\n",
    "        tuning_results.append({\n",
    "            'Model': model_name,\n",
    "            'CV_F1': search.best_score_,\n",
    "            'Test_Accuracy': acc,\n",
    "            'Test_F1': f1,\n",
    "            'Best_Params': search.best_params_\n",
    "        })\n",
    "        \n",
    "        print(f\"  Best CV F1: {search.best_score_:.4f}\")\n",
    "        print(f\"  Test F1: {f1:.4f}\")\n",
    "        print(f\"  Best Params: {search.best_params_}\")\n",
    "\n",
    "tuning_df = pd.DataFrame(tuning_results)\n",
    "print(\"\\n\u2705 Hyperparameter tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning_curves_title",
   "metadata": {},
   "source": [
    "## Step 7: Learning Curves & Bias-Variance Analysis\n",
    "\n",
    "Visualize how models perform with different training sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learning_curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curves\n",
    "print(\"=\"*60)\n",
    "print(\"LEARNING CURVES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Use best tuned models\n",
    "models_to_plot = {\n",
    "    'RandomForest': tuned_models.get('RandomForest', RandomForestClassifier(random_state=RANDOM_SEED)),\n",
    "    'XGBoost': tuned_models.get('XGBoost', XGBClassifier(random_state=RANDOM_SEED, eval_metric='mlogloss', use_label_encoder=False)),\n",
    "    'GradientBoosting': tuned_models.get('GradientBoosting', GradientBoostingClassifier(random_state=RANDOM_SEED))\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, model) in enumerate(models_to_plot.items()):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X_tuning, y_tuning,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        cv=5,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    train_std = train_scores.std(axis=1)\n",
    "    val_mean = val_scores.mean(axis=1)\n",
    "    val_std = val_scores.std(axis=1)\n",
    "    \n",
    "    axes[idx].plot(train_sizes, train_mean, 'o-', label='Training', color='blue')\n",
    "    axes[idx].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    axes[idx].plot(train_sizes, val_mean, 'o-', label='Validation', color='orange')\n",
    "    axes[idx].fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='orange')\n",
    "    \n",
    "    axes[idx].set_title(f'{name}\\nLearning Curve', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Training Size')\n",
    "    axes[idx].set_ylabel('F1 Macro')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    # Bias-Variance analysis\n",
    "    gap = train_mean[-1] - val_mean[-1]\n",
    "    if gap > 0.1:\n",
    "        diagnosis = 'High Variance (Overfitting)'\n",
    "    elif val_mean[-1] < 0.7:\n",
    "        diagnosis = 'High Bias (Underfitting)'\n",
    "    else:\n",
    "        diagnosis = 'Good Fit'\n",
    "    axes[idx].text(0.5, 0.02, f'Gap: {gap:.3f} - {diagnosis}', \n",
    "                   transform=axes[idx].transAxes, fontsize=9, ha='center')\n",
    "\n",
    "plt.suptitle('Learning Curves - Bias/Variance Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_curves.png', dpi=100)\n",
    "mlflow.log_artifact('learning_curves.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2705 Learning curves generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_eval_title",
   "metadata": {},
   "source": [
    "## Step 8: Final Model Evaluation\n",
    "\n",
    "Select the best model and do comprehensive evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find best model from tuning\n",
    "best_model_name = tuning_df.loc[tuning_df['Test_F1'].idxmax(), 'Model']\n",
    "best_model = tuned_models[best_model_name]\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Best Params: {tuning_df.loc[tuning_df['Test_F1'].idxmax(), 'Best_Params']}\")\n",
    "\n",
    "# Final predictions\n",
    "y_pred_final = best_model.predict(X_test_np)\n",
    "y_proba_final = best_model.predict_proba(X_test_np)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_np, y_pred_final, target_names=['Black', 'White', 'Draw']))\n",
    "\n",
    "# Confusion Matrix\n",
    "log_confusion_matrix_plot(y_test_np, y_pred_final, title=f'Best Model: {best_model_name}')\n",
    "\n",
    "# Log final model to MLflow\n",
    "with mlflow.start_run(run_name=f'BEST_{best_model_name}'):\n",
    "    mlflow.log_param('model', best_model_name)\n",
    "    mlflow.log_param('type', 'BEST_MODEL')\n",
    "    log_metrics_to_mlflow(y_test_np, y_pred_final, y_proba_final, prefix='final_')\n",
    "    mlflow.sklearn.log_model(best_model, 'best_model')\n",
    "    \n",
    "print(\"\\n\u2705 Best model logged to MLflow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "all_results_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Results Summary\n",
    "print(\"=\"*60)\n",
    "print(\"COMPLETE RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Combine all results\n",
    "all_results = pd.concat([\n",
    "    results_df.assign(Type='Base Model'),\n",
    "    ensemble_df.rename(columns={'Ensemble': 'Model'}).assign(Type='Ensemble'),\n",
    "    tuning_df[['Model', 'Test_F1']].rename(columns={'Test_F1': 'F1_Macro'}).assign(Type='Tuned', Resampling='SMOTETomek')\n",
    "], ignore_index=True)\n",
    "\n",
    "# Top 10 overall\n",
    "print(\"\\nTop 10 Models Overall (by F1 Macro):\")\n",
    "top10 = all_results.nlargest(10, 'F1_Macro')[['Model', 'Resampling', 'Type', 'F1_Macro']]\n",
    "print(top10.to_string(index=False))\n",
    "\n",
    "# Visualize top 10\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = {'Base Model': 'steelblue', 'Ensemble': 'forestgreen', 'Tuned': 'coral'}\n",
    "bars = ax.barh(range(len(top10)), top10['F1_Macro'], \n",
    "               color=[colors[t] for t in top10['Type']])\n",
    "ax.set_yticks(range(len(top10)))\n",
    "ax.set_yticklabels([f\"{row['Model']} ({row['Resampling']})\" for _, row in top10.iterrows()])\n",
    "ax.set_xlabel('F1 Macro')\n",
    "ax.set_title('Top 10 Models by F1 Macro Score', fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=c, label=t) for t, c in colors.items()]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('top10_models.png', dpi=100)\n",
    "mlflow.log_artifact('top10_models.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83c\udf89 MODELING PIPELINE COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal MLflow runs: Check mlflow ui for details\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Best F1: {tuning_df['Test_F1'].max():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}